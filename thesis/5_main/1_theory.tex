\section{Теоретичні відомості}
\subsection{Метод головних компонент}
Метод головних компонент (Principal component analysis) --- метод, що дозволяє
зменшити розмірність досліджуваної вибірки з мінімальними втратами інформації.

Маємо $m$ об’єктів, з яких треба зняти по $n$ певних властивостей.
На вході в нас є виборки $\vec{X}_k$, кожна з яких відповідає сукупності
властивостей $k$-го об’єкту
\begin{equation*}
  \vec{X}_k = \begin{bmatrix}
    x_k^1  \\
    x_k^2  \\
    \vdots \\
    x_k^n
  \end{bmatrix},
  \qquad k = \overline{1,m}
\end{equation*}
Згрупуємо всі вимірювання в одну матрицю $X$
\begin{equation*}
  X = \begin{bmatrix}
    x_1^1  & x_2^1  & \dots  & x_m^1  \\
    x_1^2  & x_2^2  & \dots  & x_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    x_1^n  & x_2^n  & \dots  & x_m^n
  \end{bmatrix}
\end{equation*}

Спочатку нам знадобиться знайти вибіркові середні значення для кожної
властивості
\begin{equation*}
  a_i = \frac{1}{m} \cdot \sum_{k=1}^{m} x_k^i, \qquad i = \overline{1,n}
\end{equation*}
Маємо вектор вибіркових середніх значень
\begin{comment}
\begin{equation*}
  \vec{a} = \begin{bmatrix}
    \frac{1}{m} \sum_{k=1}^{m} x_k^1 \\
    \frac{1}{m} \sum_{k=1}^{m} x_k^2 \\
    \vdots                           \\
    \frac{1}{m} \sum_{k=1}^{m} x_k^n \\
  \end{bmatrix}
\end{equation*}
\end{comment}
\begin{equation*}
  \vec{a} = \begin{bmatrix}
    a_1    \\
    a_2    \\
    \vdots \\
    a_n
  \end{bmatrix}
\end{equation*}

Центруємо отримані дані, що містяться в матриці $X$, віднявши від кожного
стовбця вектор вибіркових середніх $\vec{a}$
\begin{comment}
\begin{equation*}
  \tilde{x}_k^i = x_k^i - a_i,\qquad k = \overline{1,m}, i = \overline{1,n}
\end{equation*}
Кожен стовбець матриці вимірювань прийме вигляд
\begin{equation*}
  \tilde{X}_k
  = \vec{X}_k - \vec{a}
  = \begin{bmatrix}
    x_k^1 - a_1 \\
    x_k^2 - a_2 \\
    \vdots      \\
    x_k^n - a_n
  \end{bmatrix},
  \qquad k = \overline{1,m}
\end{equation*}
Маємо матрицю центрованих вимірювань $\tilde{X}$
\end{comment}
\begin{equation*}
  \tilde{X}
  = \begin{bmatrix}
    \tilde{x}_1^1  & \tilde{x}_2^1  & \dots  & \tilde{x}_m^1  \\
    \tilde{x}_1^2  & \tilde{x}_2^2  & \dots  & \tilde{x}_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{x}_1^n  & \tilde{x}_2^n  & \dots  & \tilde{x}_m^n
  \end{bmatrix}
  = \begin{bmatrix}
    x_1^1 - a_1  & x_2^1 - a_1  & \dots  & x_m^1 - a_1 \\
    x_1^2 - a_2  & x_2^2 - a_2  & \dots  & x_m^2 - a_2 \\
    \vdots       & \vdots       & \ddots & \vdots      \\
    x_1^n - a_n  & x_2^n - a_n  & \dots  & x_m^n - a_n
  \end{bmatrix}
\end{equation*}

Обчислюємо вибіркову коваріаційну матрицю властивостей.
Вибіркову коваріацію $i$ та $j$ властивості рахуємо за формулою
\begin{equation*}
  \sigma_i^j
  = \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^i \cdot \tilde{x}_k^j
  = \frac{1}{m} \cdot \sum_{k=1}^{m}
    \left[ \left( x_k^i - a_i \right) \cdot \left( x_k^j - a_j \right) \right],
    \qquad i,j = \overline{1,n}
\end{equation*}
Маємо вибіркову коваріаційну матрицю
\begin{equation*}
  K = \begin{bmatrix}
    \sigma_1^1 & \sigma_2^1 & \dots  & \sigma_n^1 \\
    \sigma_1^2 & \sigma_2^2 & \dots  & \sigma_n^2 \\
    \vdots     & \vdots     & \ddots & \vdots     \\
    \sigma_1^n & \sigma_2^n & \dots  & \sigma_n^n \\
  \end{bmatrix}
\end{equation*}
\begin{comment}
\begin{equation*}
  K =
  \begin{bmatrix}
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^n
    \\
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^n
    \\
    \vdots & \vdots & \dots & \vdots \\
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^n
  \end{bmatrix}
\end{equation*}
\end{comment}

Щоб отримувати лише потрібну інформацію, ми хочемо знайти таке ортогональне
лінійне перетворення $L$ вхідної матриці $\tilde{X}$, щоб отримати матрицю
$Y = L \cdot \tilde{X}$, яка має діагональну вибіркову ковариаційну матрицю $K'$
з незростаючими зверху вниз значеннями.
Діагональна вибіркова коваріаційна матриця гарантує той факт, що отримані
значення $Y$ будуть некорельованими.
Рангування значень діагональних елементів матриці $K'$ за величиною дасть більш
наглядне представлення про будову досліджуваних об’єктів, адже діагональні
елементи --- вибіркові дисперсії; а чим більше дисперсія, тим більше відповідна
властивість змінюється від об’єкту до об’єкту і тим більше корисної інформації
вона нам надає.

Вибіркова коваріаційна матриця $K'$ для $Y = L \cdot \tilde{X}$ має вигляд
\begin{equation*}
  K'
  = L \cdot K \cdot L^*
  = \begin{bmatrix}
    \lambda_1 & 0         & \dots  & 0      \\
    0         & \lambda_2 & \dots  & 0      \\
    \vdots    & \vdots    & \ddots & \vdots \\
    0         & 0         & \dots  & \lambda_n
  \end{bmatrix}
\end{equation*}
З лінійної алгебри відомо, що матриця $L$ складається з координат власних
векторів матриці $K$, а елементи $\lambda_k$ --- її власні числа, які
існують і є невід’ємними через невід’ємну означеність матриці $K$.
Вважаємо що числа $\lambda_1, \dots, \lambda_n$ впорядковані від більшого до
меншого для зручності подальших дій.
Позначимо власний вектор матриці $K$, що відповідає власному числу $\lambda_k$,
як $\vec{l}_k$. Тоді
\begin{equation*}
  \vec{l}_k
  = \left[ l_k^1, l_k^2, \dots, l_k^n \right],
  \qquad k = \overline{1,n}
\end{equation*}
Матриця $L$ має вигляд
\begin{equation*}
  L = \begin{bmatrix}
    l_1^1  & l_1^2  & \dots  & l_1^n  \\
    l_2^1  & l_2^2  & \dots  & l_2^n  \\
    \vdots & \vdots & \ddots & \vdots \\
    l_n^1  & l_n^2  & \dots  & l_n^n  \\
  \end{bmatrix}
\end{equation*}

Треба зменшити розмірність простору досліджуваних параметрів системи з $n$ до
$p<n$, але при цьому втратити якомога менше відомостей про досліджувані
об’єкти.
Введемо міру інформації, що залишається при зменшенні кількості компонент, що
розглядаються
\begin{equation*}
  I = \frac{\lambda_1 + \dots + \lambda_p}{\lambda_1 + \dots + \lambda_n}
\end{equation*}
Будемо вважати, що діємо продуктивно, тому починаємо обирати з перших
компонент, адже саме вони є найбільш інформативними.
Також бачимо, що інформативність змінюється в межах від $0$
(нічого не дізнаємось) до $1$ (зберегли усю інформацію).

\subsection{Гістограма}

Для подальшого аналізу потрібно здобути щільність розподілення.
Оскільки маємо справу з вибіркою і вибірковими характеристиками,
потрібно побудувати гістограму, адже це і є вибіркова характеристика,
що відповідає щільності.

Побудуємо $i$-й стовбець гістограми для виборки з $k$-ї строки матриці $Y$
\begin{equation*}
  h_i^k = \frac{1}{m} \cdot \sum_{j=1}^{m} \indicator{y_j^k \in I_i^k}
\end{equation*}
де $I_k$ --- набір напівінтервалів, що розбиває відрізок
$\left[ \min\limits_{j=\overline{1,m}}{y_j^k};
\max\limits_{j=\overline{1,m}}{y_j^k} \right]$ на $N$ рівних частин.
Для вибору $N$ можна скористатися досить відомою формулою Стьорджеса
(Sturges' formula) \cite{Sturges:1926:CCI}
\begin{equation*}
  N = \lfloor \log_2 m \rfloor + 1
\end{equation*}
Маємо матрицю гістограм
\begin{equation*}
  H = \begin{bmatrix}
    h_1^1  & h_2^1  & \dots  & h_N^1  \\
    h_1^2  & h_2^2  & \dots  & h_N^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    h_1^n  & h_2^n  & \dots  & h_N^n
  \end{bmatrix}
\end{equation*}
і напівінтервалів, що відповідають кожному стовбчику кожної гістограми
\begin{equation*}
  I = \begin{bmatrix}
    I_1^1  & I_2^1  & \dots  & I_N^1  \\
    I_1^2  & I_2^2  & \dots  & I_N^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    I_1^n  & I_2^n  & \dots  & I_N^n
  \end{bmatrix}
\end{equation*}

\subsection{Критерій узгодженості Пірсона $\chi^2$}
Гістограма може використовуватися не тільки для графічної інтерпретації
отриманих даних, але й для віднесення вибірки до якогось відомого розподілу.
Відповідь на питання ``Чи дійсно вибірка $y_1^k$, $\dots$, $y_m^k$ має розподіл
$F$?'' може надати критерій узгодженості Пірсона.

Спочатку треба визначити ймовірність $p_i^k$ того, що випадкова величина $\xi^k$
з функцією розподілу $F^k$ попаде в $I_i^k$
\begin{equation*}
  p_i^k = \probability{\xi \in I_i^k},
  \qquad \probability{\xi^k \le x} = \cdfof{k}{x}
\end{equation*}
Далі рахуємо значення $R^k$ за формулою
\begin{equation*}
  R^k
  = \sum_{i=1}^{N}\frac{\left( h_i^k - m \cdot p_i^k \right)^2}{m \cdot p_i^k}
\end{equation*}
За допомогою таблиці знаходимо приблизний розв’язок наступного рівняння відносно
$r_{\alpha}$
\begin{equation*}
  \probability{\chi_{N-1}^2 \le r_{\alpha}} = \alpha
\end{equation*}
Якщо $R^k \le r_{\alpha}$, то гіпотеза про те, що вибірка $Y^k$ дійсно має
розподіл $F$, не відхиляється. В противному випадку значення $R^k$ буде
поподитись як $\sqrt{m}$, а гіпотезу буде відхилено.
