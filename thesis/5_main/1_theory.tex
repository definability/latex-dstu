\section{Теоретичні відомості}
\subsection{Метод головних компонент}
Метод головних компонент (Principal component analysis) --- метод, що дозволяє
зменшити розмірність досліджуваної вибірки з мінімальними втратами інформації.

Маємо $m$ об’єктів, з яких треба зняти по $n$ певних властивостей.
На вході в нас є виборки $\vec{X}_k$, кожна з яких відповідає сукупності
властивостей $k$-го об’єкту
\begin{equation*}
  \vec{X}_k = \begin{bmatrix}
    x_k^1  \\
    x_k^2  \\
    \vdots \\
    x_k^n
  \end{bmatrix},
  \qquad k = \overline{1,m}
\end{equation*}
Згрупуємо всі вимірювання в одну матрицю $X$
\begin{equation*}
  X = \begin{bmatrix}
    x_1^1  & x_2^1  & \dots  & x_m^1  \\
    x_1^2  & x_2^2  & \dots  & x_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    x_1^n  & x_2^n  & \dots  & x_m^n
  \end{bmatrix}
\end{equation*}

Спочатку нам знадобиться знайти вибіркові середні значення для кожної
властивості
\begin{equation*}
  a_i = \frac{1}{m} \cdot \sum_{k=1}^{m} x_k^i, \qquad i = \overline{1,n}
\end{equation*}
Маємо вектор вибіркових середніх значень
\begin{comment}
\begin{equation*}
  \vec{a} = \begin{bmatrix}
    \frac{1}{m} \sum_{k=1}^{m} x_k^1 \\
    \frac{1}{m} \sum_{k=1}^{m} x_k^2 \\
    \vdots                           \\
    \frac{1}{m} \sum_{k=1}^{m} x_k^n \\
  \end{bmatrix}
\end{equation*}
\end{comment}
\begin{equation*}
  \vec{a} = \begin{bmatrix}
    a_1    \\
    a_2    \\
    \vdots \\
    a_n
  \end{bmatrix}
\end{equation*}

Центруємо отримані дані, що містяться в матриці $X$, віднявши від кожного
стовбця вектор вибіркових середніх $\vec{a}$
\begin{comment}
\begin{equation*}
  \tilde{x}_k^i = x_k^i - a_i,\qquad k = \overline{1,m}, i = \overline{1,n}
\end{equation*}
Кожен стовбець матриці вимірювань прийме вигляд
\begin{equation*}
  \tilde{X}_k
  = \vec{X}_k - \vec{a}
  = \begin{bmatrix}
    x_k^1 - a_1 \\
    x_k^2 - a_2 \\
    \vdots      \\
    x_k^n - a_n
  \end{bmatrix},
  \qquad k = \overline{1,m}
\end{equation*}
Маємо матрицю центрованих вимірювань $\tilde{X}$
\end{comment}
\begin{equation*}
  \tilde{X}
  = \begin{bmatrix}
    \tilde{x}_1^1  & \tilde{x}_2^1  & \dots  & \tilde{x}_m^1  \\
    \tilde{x}_1^2  & \tilde{x}_2^2  & \dots  & \tilde{x}_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{x}_1^n  & \tilde{x}_2^n  & \dots  & \tilde{x}_m^n
  \end{bmatrix}
  = \begin{bmatrix}
    x_1^1 - a_1  & x_2^1 - a_1  & \dots  & x_m^1 - a_1 \\
    x_1^2 - a_2  & x_2^2 - a_2  & \dots  & x_m^2 - a_2 \\
    \vdots       & \vdots       & \ddots & \vdots      \\
    x_1^n - a_n  & x_2^n - a_n  & \dots  & x_m^n - a_n
  \end{bmatrix}
\end{equation*}

Обчислюємо вибіркову коваріаційну матрицю властивостей.
Вибіркову коваріацію $i$ та $j$ властивості рахуємо за формулою
\begin{equation*}
  \sigma_i^j
  = \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^i \cdot \tilde{x}_k^j
  = \frac{1}{m} \cdot \sum_{k=1}^{m}
    \left[ \left( x_k^i - a_i \right) \cdot \left( x_k^j - a_j \right) \right],
    \qquad i,j = \overline{1,n}
\end{equation*}
Маємо вибіркову коваріаційну матрицю
\begin{equation*}
  K = \begin{bmatrix}
    \sigma_1^1 & \sigma_2^1 & \dots  & \sigma_n^1 \\
    \sigma_1^2 & \sigma_2^2 & \dots  & \sigma_n^2 \\
    \vdots     & \vdots     & \ddots & \vdots     \\
    \sigma_1^n & \sigma_2^n & \dots  & \sigma_n^n \\
  \end{bmatrix}
\end{equation*}
\begin{comment}
\begin{equation*}
  K =
  \begin{bmatrix}
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^1 \cdot \tilde{x}_k^n
    \\
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^2 \cdot \tilde{x}_k^n
    \\
    \vdots & \vdots & \dots & \vdots \\
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^1
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^2
    &
    \dots
    &
    \frac{1}{m} \cdot \sum_{k=1}^{m} \tilde{x}_k^n \cdot \tilde{x}_k^n
  \end{bmatrix}
\end{equation*}
\end{comment}

Щоб отримувати лише потрібну інформацію, ми хочемо знайти таке ортогональне
лінійне перетворення $L$ вхідної матриці $\tilde{X}$, щоб отримати матрицю
$Y = L \cdot \tilde{X}$, яка має діагональну вибіркову ковариаційну матрицю $K'$
з незростаючими зверху вниз значеннями.
Діагональна вибіркова коваріаційна матриця гарантує той факт, що отримані
значення $Y$ будуть некорельованими.
Рангування значень діагональних елементів матриці $K'$ за величиною дасть більш
наглядне представлення про будову досліджуваних об’єктів, адже діагональні
елементи --- вибіркові дисперсії; а чим більше дисперсія, тим більше відповідна
властивість змінюється від об’єкту до об’єкту і тим більше корисної інформації
вона нам надає.

Вибіркова коваріаційна матриця $K'$ для $Y = L \cdot \tilde{X}$ має вигляд
\begin{equation*}
  K'
  = L \cdot K \cdot L^*
  = \begin{bmatrix}
    \lambda_1 & 0         & \dots  & 0      \\
    0         & \lambda_2 & \dots  & 0      \\
    \vdots    & \vdots    & \ddots & \vdots \\
    0         & 0         & \dots  & \lambda_n
  \end{bmatrix}
\end{equation*}
З лінійної алгебри відомо, що матриця $L$ складається з координат власних
векторів матриці $K$, а елементи $\lambda_k$ --- її власні числа, які
існують і є невід’ємними через невід’ємну означеність матриці $K$.
Вважаємо що числа $\lambda_1, \dots, \lambda_n$ впорядковані від більшого до
меншого для зручності подальших дій.
Позначимо власний вектор матриці $K$, що відповідає власному числу $\lambda_k$,
як $\vec{l}_k$. Тоді
\begin{equation*}
  \vec{l}_k
  = \left[ l_k^1, l_k^2, \dots, l_k^n \right],
  \qquad k = \overline{1,n}
\end{equation*}
Матриця $L$ має вигляд
\begin{equation*}
  L = \begin{bmatrix}
    l_1^1  & l_1^2  & \dots  & l_1^n  \\
    l_2^1  & l_2^2  & \dots  & l_2^n  \\
    \vdots & \vdots & \ddots & \vdots \\
    l_n^1  & l_n^2  & \dots  & l_n^n  \\
  \end{bmatrix}
\end{equation*}

Треба зменшити розмірність простору досліджуваних параметрів системи з $n$ до
$p<n$, але при цьому втратити якомога менше відомостей про досліджувані
об’єкти.
Введемо міру інформації, що залишається при зменшенні кількості компонент, що
розглядаються
\begin{equation*}
  I = \frac{\lambda_1 + \dots + \lambda_p}{\lambda_1 + \dots + \lambda_n}
\end{equation*}
Будемо вважати, що діємо продуктивно, тому починаємо обирати з перших
компонент, адже саме вони є найбільш інформативними.
Також бачимо, що інформативність змінюється в межах від $0$
(нічого не дізнаємось) до $1$ (зберегли усю інформацію).

Надалі буде розглядатися матриця головних компонент $Y$
\begin{equation*}
  Y = \begin{bmatrix}
    y_1^1  & y_2^1  & \dots  & y_m^1  \\
    y_1^2  & y_2^2  & \dots  & y_m^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^p  & y_2^p  & \dots  & y_m^p  \\
    \end{bmatrix}
\end{equation*}
\subsection{Гістограма}

Для подальшого аналізу потрібно здобути щільність розподілу головних компонент.
Оскільки маємо справу з вибіркою і вибірковими характеристиками,
потрібно побудувати гістограму, адже це і є вибіркова характеристика,
що відповідає щільності.

Побудуємо $j$-й стовбець гістограми для виборки з $k$-ї строки матриці $Y$
\begin{equation*}
  h_j^k = \frac{1}{m} \cdot \sum_{i=1}^{m} \indicator{y_i^k \in I_j^k},
  \qquad j = \overline{1, N},
  \qquad k = \overline{1, p}
\end{equation*}
де $I^k$ --- набір напівінтервалів, що розбиває відрізок
$\left[ \min\limits_{i=\overline{1,m}}{y_i^k};
\max\limits_{i=\overline{1,m}}{y_i^k} \right]$ на $N$ рівних частин.
Для вибору $N$ можна скористатися досить відомою формулою Стьорджеса
(Sturges' formula) \cite{Sturges:1926:CCI}
\begin{equation*}
  N = \lfloor \log_2 m \rfloor + 1
\end{equation*}
Маємо матрицю гістограм
\begin{equation*}
  H = \begin{bmatrix}
    h_1^1  & h_2^1  & \dots  & h_N^1  \\
    h_1^2  & h_2^2  & \dots  & h_N^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    h_1^p  & h_2^p  & \dots  & h_N^p
  \end{bmatrix}
\end{equation*}
і напівінтервалів, що відповідають кожному стовбчику кожної гістограми
\begin{equation*}
  I = \begin{bmatrix}
    I_1^1  & I_2^1  & \dots  & I_N^1  \\
    I_1^2  & I_2^2  & \dots  & I_N^2  \\
    \vdots & \vdots & \ddots & \vdots \\
    I_1^p  & I_2^p  & \dots  & I_N^p
  \end{bmatrix}
\end{equation*}

\subsection{Поліноміальний розподіл}
Введемо матрицю частот $\nu$
\begin{equation*}
  \nu = m \cdot H
\end{equation*}
Кожна компонента --- кількість елементів вибірки, що потрапили у відповідний
напівінтервал
\begin{equation*}
  \nu_j^k = \sum_{i=1}^{m} \indicator{y_i^k \in I_j^k},
  \qquad j = \overline{1, N},
  \qquad k = \overline{1, p}
\end{equation*}

Розглянемо вектор
\begin{equation}\label{eq:vector:multinomial}
  \nu^k = \left[ \nu_1^k, \dots, \nu_N^k \right],
  \qquad k = \overline{1,p}
\end{equation}
Маємо серію з $m$ незалежних експериментів, кожен з яких може закінчитися
одним з $N$ результатів $E_1^k$, $\dots$, $E_N^k$, що взаємно виключаються
\begin{equation*}
  \probability{E_i^k \cap E_j^k} = 0,
  %\qquad E_i^k \neq E_j^k
  \qquad i \neq j,
  \qquad k = \overline{1,p}
\end{equation*}
Якщо випадкові величини $y_i^k$ мають заздалегіть відомий
розподіл, який однаковий в межах однієї строки $Y^k$, маємо ймовірності
кожного результату експерименту
\begin{equation*}
  \rho_j^k = \probability{E_j^k} = \probability{y_1^k \in I_j^k}
  = \dots = \probability{y_m^k \in I_j^k},
  \qquad j = \overline{1,N},
  \qquad k = \overline{1,p}
\end{equation*}
Ймовірність того, що вектор $\nu^k$ буде дорівнювати вектору
$X = \left[ x_1, \dots, x_N \right]$, рахується за формулою
\begin{equation*}
  f_{multi}^k\left( X \right)
  = \probability{\nu^k = X}
  = m! \cdot \prod_{j=1}^N \frac{\left( \rho_j^k \right)^{x_j}}{x_j!},
  \qquad \sum_{j=1}^{N}x_j = m
\end{equation*}

Математичне сподівання і дисперсія кожного елементу співпадає з математичним
сподіванням і дисперсією біноміального розподілу з відповідними
характеристиками, адже випадкові величини $\nu_j^k$ не залежать одна від одної
\begin{equation*}
  \mean{\nu_j^k} = N \cdot \rho_j^k,\qquad
  \dispersion{\nu_j^k} = N \cdot \rho_j^k \cdot \left( 1 - \rho_j^k \right),
  \qquad j = \overline{1,N},
  \qquad k = \overline{1,p}
\end{equation*}
Коваріація двох різних елементів вектора $\nu^k$ рахується за формулою
\cite{Mukhopadhyay:2000}
\begin{equation*}
  \cov{\nu_i^k, \nu_j^k} = - N \cdot \rho_i^k \cdot \rho_j^k,
  \qquad i \neq j
  %\qquad k = \overline{1,p},
  %\qquad i,j = \overline{1,N},
\end{equation*}
Введемо дельта-функцію
\begin{equation*}
  \delta_{i, j} = \begin{cases}
    1,& i = j \\
    0,& i \neq j
  \end{cases}
\end{equation*}
Отримаємо загальну формулу для коваріації
\begin{equation*}
  \cov{\nu_i^k, \nu_j^k}
  = N \cdot \rho_i^k \cdot \left( \delta_{i, j} - \rho_j^k \right)
\end{equation*}
Отже, коваріаційна матриця $A^k$ вектора $\nu^k$ виглядає наступним чином
\begin{equation*}
  A^k
  = \left| \cov{\nu_i^k, \nu_j^k} \right|_{i,j=1}^N
  = N \cdot
   \begin{bmatrix}
      \rho_1^k \cdot \left( 1 - \rho_1^k \right) & - \rho_1^k \cdot \rho_2^k
              & \cdots & - \rho_1^k \cdot \rho_N^k \\

      - \rho_2^k \cdot \rho_1^k                  & \rho_2^k \cdot
          \left( 1 - \rho_2^k \right) & \cdots & - \rho_2^k \cdot \rho_N^k \\

      \vdots                                     & \vdots
              & \ddots & \vdots \\

       - \rho_N^k \cdot \rho_1^k                 & - \rho_N^k \cdot \rho_2^k
              & \cdots & \rho_N^k \cdot \left( 1 - \rho_N^k \right)
    \end{bmatrix}
\end{equation*}
Додамо всі її строки до першої
\begin{equation*}
  A^k
  = N \cdot
   \begin{bmatrix}
     \rho_1^k \cdot \left( 1 - \sum_{j}^{N}\rho_j^k \right) &
     \rho_2^k \cdot \left( 1 - \sum_{j}^{N}\rho_j^k \right)
     & \cdots & \rho_N^k \cdot \left( 1 - \sum_{j}^{N}\rho_j^k \right) \\

      - \rho_2^k \cdot \rho_1^k                  & \rho_2^k \cdot
          \left( 1 - \rho_2^k \right) & \cdots & - \rho_2^k \cdot \rho_N^k \\

      \vdots                                     & \vdots
              & \ddots & \vdots \\

       - \rho_N^k \cdot \rho_1^k                 & - \rho_N^k \cdot \rho_2^k
              & \cdots & \rho_N^k \cdot \left( 1 - \rho_N^k \right)
    \end{bmatrix}
\end{equation*}
Пам’ятаємо те, що сума ймовірностей дорівнює одиниці
\begin{equation*}
  \sum_{j=1}^{N}\rho_j^k = 1
\end{equation*}
Отже
\begin{equation*}
  A^k
  = N \cdot
   \begin{bmatrix}
     0 & 0 & \cdots & 0 \\
      - \rho_2^k \cdot \rho_1^k                  & \rho_2^k \cdot
          \left( 1 - \rho_2^k \right) & \cdots & - \rho_2^k \cdot \rho_N^k \\

      \vdots                                     & \vdots
              & \ddots & \vdots \\

       - \rho_N^k \cdot \rho_1^k                 & - \rho_N^k \cdot \rho_2^k
              & \cdots & \rho_N^k \cdot \left( 1 - \rho_N^k \right)
    \end{bmatrix}
\end{equation*}
Отже, матриця $A$ є виродженою і має ранг $N-1$ \cite{Kendall:1966}.

Лагранжем було показано, що функція ймовірності поліноміального розподілу,
помножена на $\sqrt{m^{N-1}}$, прямує до щільності ймовірності нормального
вектора, що має $N-1$ компонент, де константа $\sqrt{2 \cdot \pi}$ з’являється
завдяки застосуванню формули Стірлінґа \cite{Lagrange:1868}
\begin{equation}\label{eq:probability:normal}
  m^{\frac{N-1}{2}} \cdot f_{multi}^k\left( X \right)
  \Covergencen{m} f_{norm}^k\left( H^k \right)
  = \frac{\exp{\left\{ -\frac{1}{2} \cdot \sum_{j=1}^{N}
        \frac{m \cdot \left( \rho_j^k - h_j^k \right)^2}{h_j^k} \right\}}}
      {\sqrt{\left( 2 \cdot \pi \right)^{N-1} \cdot h_1^k \cdots h_N^k}}
\end{equation}

\subsection{Критерій узгодженості Пірсона $\chi^2$}
Гістограма може використовуватися не тільки для графічної інтерпретації
отриманих даних, але й для віднесення вибірки до якогось відомого розподілу.
Відповідь на питання ``Чи дійсно вибірка $y_1^k$, $\dots$, $y_p^k$ має розподіл
$F^k$?'' може надати критерій узгодженості Пірсона. %\cite{Pearson:1900}.

Знову розглянемо вектор \eqref{eq:vector:multinomial}
\begin{equation*}
  \nu^k = \left[ \nu_1^k, \dots, \nu_N^k \right],
  \qquad k = \overline{1,p}
\end{equation*}
Пам’ятаємо, що функція ймовірності цього вектора прямує до функції ймовірності
нормального $\left( N - 1 \right)$-вимірного вектора.
Розглянемо квадратичну форму, що знаходиться в експоненті
\eqref{eq:probability:normal}
\begin{equation}\label{eq:pearson:Rk}
  R^k
  = \sum_{j=1}^{N}\frac{\left( m \cdot \rho_j^k - \nu_j^k \right)^2}{\nu_j^k}
  = m \cdot \sum_{j=1}^{N}\frac{\left( \rho_j^k - h_j^k \right)^2}{h_j^k}
\end{equation}
Спираючись на результати, отримані Лагранжем, Пірсон показав, що квадратична
форма має розподіл $\chi^2$ з $N-1$ ступенями вільності
\cite{Pearson:1900} \cite{Hald:2010}.

З таблиці для функції розподілу $\chi_{N-1}^2$ обираємо рівень значущості
$\alpha$ і шукаємо відповідне до кількості ступенів вільності $r_{\alpha}$.
Рівень значущості --- ймовірність помилки першого роду, тобто ймовірність того,
що буде відкинуто вірну гіпотезу
\begin{equation*}
  \probability{\chi_{N-1}^2 \ge r_{\alpha}} = \alpha
\end{equation*}
Якщо $R^k \le r_{\alpha}$, то гіпотеза про те, що вибірка $Y^k$ дійсно має
розподіл $F^k$, не відхиляється.

Без втрати загальності розглянемо той випадок, коли ймовірність $\rho_i^k$
відгадана невірно. Повернемося до формули \eqref{eq:pearson:Rk}
\begin{equation*}
  R^k
  = \sum_{j=1}^{N}\frac{\left( m \cdot \rho_j^k - \nu_j^k \right)^2}{\nu_j^k}
\end{equation*}
Всі члени суми є невід’ємними. Якщо хоча б один елемент буде завеликим,
то великою буде вся сума. Маємо випадкову величину $\eta$
\begin{equation*}
  \eta
  = \nu_i^k - m \cdot \rho_i^k
  = \sum_{j=1}^{m} \left( \xi_j - \rho_i^k \right),
  \qquad \indicator{y_j^k \in I_i^k} = \xi_j
\end{equation*}
Якщо $\rho_i^k$ вгадано невірно, то воно не дорівнює математичному очікуванню
індикатора. Додамо та віднімемо справжнє математичне очікування
\begin{equation*}
  \eta
  = \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} + \mean{\xi_1} - \rho_i^k \right)
  = \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} \right)
    + \sum_{j=1}^{m} \left( \mean{\xi_1} - \rho_i^k \right)
\end{equation*}
Останній доданок є просто різницею, помноженою на $m$
\begin{equation*}
  \eta
  = \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} \right)
    + m \cdot \left( \mean{\xi_1} - \rho_i^k \right)
\end{equation*}
Поділимо на $\sqrt{m}$, щоб скористатися центральною граничною теоремою
\begin{equation*}
  \frac{\eta}{\sqrt{m}}
  = \frac{1}{\sqrt{m}} \cdot \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} \right)
    + \frac{1}{\sqrt{m}} \cdot m \cdot \left( \mean{\xi_1} - \rho_i^k \right)
\end{equation*}
Перший доданок має розподіл $N\left( 0, \sigma^2 \right)$, де $\sigma^2$ ---
дисперсія випадкової величини $\xi_1$ для достатньо великих $m$.
Отже, вся сума зростає пропорційно
$\sqrt{m}$
\begin{equation*}
  \frac{\eta}{\sqrt{m}}
  = \frac{1}{\sqrt{m}} \cdot \sum_{j=1}^{m} \left( \xi_j - \mean{\xi_1} \right)
    + \sqrt{m} \cdot \left( \mean{\xi_1} - \rho_i^k \right)
  \sim \sqrt{m} \cdot \left( \mean{\xi_1} - \rho_i^k \right)
\end{equation*}
Тобто, якщо гіпотеза невірна, то $R^k$ буде зростати пропорційно до величини
$\sqrt{m}$, що є достатньо великою швидкістю.

Чим більше рівень значущості, тим менше значення $r_{\alpha}$, а отже і
проміжок, в який дозволяється потрапити значенню $R^k$.
В цьому випадку зростає ймовірність відхилити вірну гіпотезу щодо розподілу,
але при цьому є більше впевненості в правильності результату.
Зазвичай $\alpha$ обирають рівним $0.1$, $0.05$, $0.01$.
